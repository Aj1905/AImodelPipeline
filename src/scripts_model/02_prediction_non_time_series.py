"""
ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸäºˆæ¸¬å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿè¡Œä¾‹:
python src/scripts_model/02_prediction_non_time_series.py \
    --db-path data/database.sqlite \
    --predict-table new_data \
    --output-table predictions

python src/scripts_model/02_prediction_non_time_series.py \
    --db-path data/database.sqlite \
    --predict-table sales_test \
    --output-table sales_predictions

python src/scripts_model/02_prediction_non_time_series.py \
    --db-path data/database.sqlite \
    --predict-table customer_new \
    --output-table customer_predictions

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    --db-path: SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    --predict-table: å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆç›®çš„å¤‰æ•°åˆ—ã¯ä¸è¦ï¼‰
    --output-table: äºˆæ¸¬çµæœã‚’æ›¸ãè¾¼ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«å
"""

import argparse
import json
import sqlite3
from pathlib import Path

import joblib
import pandas as pd
import polars as pl

# ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
ARTIFACT_DIR = Path("artifacts")

# æ—¢å­˜ã®åˆ—ã‚’ç‰¹å¾´é‡ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ãƒªã‚¹ãƒˆï¼ˆ01_model_non_time_series.pyã¨åŒã˜ï¼‰
FEATURE_COLUMNS = [
    "date_and_time",
    "prev_year_same_weekday_sales_daily_sum",
    "Yokohama_Sea_Level_Pressure",
    "time",
    "prev_year_same_date_sales_daily_sum",
    "time_flag",
    "date_day",
    "sales_trailing_ma_11",
    "Yokohama_Temperature",
    "Yokohama_Wind_Speed",
    "prev_year_same_weekday_sales",
    "sales_trailing_ma_1",
    "prev_year_same_weekday_sales_monthly_sum",
    "Yokohama_Relative_Humidity",
    "sales_trailing_ma_3",
    "prev_year_same_date_sales",
]

# feature_engineeringé–¢æ•°ã§ä½œæˆã™ã‚‹ç‰¹å¾´é‡ã®åå‰ï¼ˆ01_model_non_time_series.pyã¨åŒã˜ï¼‰
ENGINEERED_FEATURES = [
    "is_lunch",
    "weekday",
    "time_category",
    "season"
]

def feature_engineering(data: pl.DataFrame) -> pl.DataFrame:
    """ä¸€éƒ¨ã®åˆ—ã‚’å¤‰æ›ã—ã€æ–°ãŸãªç‰¹å¾´é‡åˆ—ã‚’è¿”ã™ï¼ˆ01_model_non_time_series.pyã¨åŒã˜ï¼‰"""
    df = data.clone()

    # æ—¥ä»˜ã‚’datetimeå‹ã«å¤‰æ›
    df = df.with_columns(pl.col("date_and_time").str.to_datetime("%Y-%m-%d %H:%M:%S").alias("datetime"))

    # æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œæˆ
    engineered = df.with_columns(
        [
            pl.when(pl.col("time").is_in([11, 12, 13])).then(1).otherwise(0).alias("is_lunch"),
            pl.col("datetime").dt.weekday().alias("weekday"),
            pl.when(pl.col("time") < 12)
            .then(0)
            .when(pl.col("time") < 15)
            .then(1)
            .when(pl.col("time") < 18)
            .then(2)
            .otherwise(3)
            .alias("time_category"),
        ]
    )

    # date_monthåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿seasonç‰¹å¾´é‡ã‚’ä½œæˆ
    if "date_month" in df.columns:
        engineered = engineered.with_columns([
            pl.when(pl.col("date_month").is_in([12, 1, 2]))
            .then(0)
            .when(pl.col("date_month").is_in([3, 4, 5]))
            .then(1)
            .when(pl.col("date_month").is_in([6, 7, 8]))
            .then(2)
            .otherwise(3)
            .alias("season"),
        ])
    else:
        # date_monthåˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å­£ç¯€ã‚’0ã§åˆæœŸåŒ–
        engineered = engineered.with_columns(pl.lit(0).alias("season"))

    # æ¬ æå€¤å‡¦ç†
    result = engineered.select(ENGINEERED_FEATURES)
    
    # Polarsã§æ¬ æå€¤ã‚’å‡¦ç†
    for col in ENGINEERED_FEATURES:
        if col in result.columns:
            # æ•°å€¤åˆ—ã®å ´åˆã¯ä¸­å¤®å€¤ã§è£œå®Œ
            result = result.with_columns(
                pl.col(col).fill_null(pl.col(col).median()).alias(col)
            )

    return result

def main():
    parser = argparse.ArgumentParser(description="ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã™")
    parser.add_argument("--db-path",       type=str, required=True, help="SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--predict-table", type=str, required=True, help="å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆç›®çš„å¤‰æ•°åˆ—ã¯ä¸è¦ï¼‰")
    parser.add_argument("--output-table",  type=str, required=True, help="äºˆæ¸¬çµæœã‚’æ›¸ãè¾¼ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«å")
    args = parser.parse_args()

    # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆèª­ã¿è¾¼ã¿
    fm       = joblib.load(ARTIFACT_DIR/"feature_manager.pkl")
    tm       = joblib.load(ARTIFACT_DIR/"target_manager.pkl")
    model    = joblib.load(ARTIFACT_DIR/"model.pkl")
    metadata = json.load(open(ARTIFACT_DIR/"metadata.json", encoding="utf-8"))

    print("ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±:")
    print(f"   å…ƒã®ç‰¹å¾´é‡: {metadata.get('original_features', [])}")
    print(f"   ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ç‰¹å¾´é‡: {metadata.get('engineered_features', [])}")

    # æœ¬ç•ªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    conn = sqlite3.connect(args.db_path)
    df = pd.read_sql(f"SELECT * FROM {args.predict_table}", conn)
    conn.close()
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­è¾¼å®Œäº†: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

    # æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
    available_features = [col for col in FEATURE_COLUMNS if col in df.columns]
    missing_features = [col for col in FEATURE_COLUMNS if col not in df.columns]

    if missing_features:
        print(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_features}")

    print(f"ğŸ“Š ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {available_features}")

    # æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
    df_selected = df[available_features].copy()

    # æ¬ æå€¤å‡¦ç†
    print("ğŸ”§ æ¬ æå€¤å‡¦ç†å®Ÿè¡Œä¸­...")
    for col in df_selected.columns:
        if df_selected[col].dtype in ['int64', 'float64']:
            # æ•°å€¤åˆ—ã®å ´åˆ
            if df_selected[col].isnull().sum() > 0:
                median_val = df_selected[col].median()
                df_selected[col] = df_selected[col].fillna(median_val)
                print(f"  {col}: ä¸­å¤®å€¤ ({median_val:.2f}) ã§è£œå®Œ")
        else:
            # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®å ´åˆ
            if df_selected[col].isnull().sum() > 0:
                mode_val = df_selected[col].mode().iloc[0] if not df_selected[col].mode().empty else 'Unknown'
                df_selected[col] = df_selected[col].fillna(mode_val)
                print(f"  {col}: æœ€é »å€¤ ({mode_val}) ã§è£œå®Œ")

    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    df_pl = pl.from_pandas(df_selected)
    engineered_features = feature_engineering(df_pl)
    engineered_features_pd = engineered_features.to_pandas()

    # å…ƒã®ç‰¹å¾´é‡ã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’çµåˆ
    X_input = pd.concat([df_selected[available_features], engineered_features_pd], axis=1)

    print(f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {len(X_input.columns)} åˆ—")
    print(f"   å…ƒã®ç‰¹å¾´é‡: {len(available_features)} åˆ—")
    print(f"   ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ç‰¹å¾´é‡: {len(ENGINEERED_FEATURES)} åˆ—")

    # å‰å‡¦ç† â†’ äºˆæ¸¬
    X_processed = fm.transform(X_input)
    y_pred = model.predict(X_processed)

    # äºˆæ¸¬çµæœã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
    target_col = metadata.get("target_column", "sales")
    df[target_col] = y_pred

    # çµæœæ›¸ãæˆ»ã—
    conn = sqlite3.connect(args.db_path)
    df.to_sql(args.output_table, conn, if_exists="replace", index=False)
    conn.close()

    print(f"âœ… äºˆæ¸¬çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ« '{args.output_table}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("ğŸ“Š äºˆæ¸¬å€¤ã®çµ±è¨ˆ:")
    print(f"   å¹³å‡: {y_pred.mean():.2f}")
    print(f"   æ¨™æº–åå·®: {y_pred.std():.2f}")
    print(f"   æœ€å°å€¤: {y_pred.min():.2f}")
    print(f"   æœ€å¤§å€¤: {y_pred.max():.2f}")

if __name__ == "__main__":
    main()
