from collections.abc import Callable
from typing import Any
from datetime import datetime

import pandas as pd
import optuna
import mlflow
from sklearn.model_selection import cross_val_score


class OptunaHyperTuner:
    """Hyper parameter tuning using Optuna with nested cross validation and MLflow integration."""

    def __init__(
        self,
        estimator_factory: Callable[[], Any],
        param_ranges: dict[str, Any],
        outer_splits: int = 5,
        inner_splits: int = 3,
        n_trials: int = 50,
        scoring: str = "r2",
        random_state: int = 42,
        use_mlflow: bool = True,
        experiment_name: str = "hyperparameter_tuning",
        run_name: str = None,
        start_mlflow_run: bool = False,
    ) -> None:
        self.estimator_factory = estimator_factory
        self.param_ranges = param_ranges
        self.outer_splits = outer_splits
        self.inner_splits = inner_splits
        self.n_trials = n_trials
        self.scoring = scoring
        self.random_state = random_state
        self.use_mlflow = use_mlflow
        self.experiment_name = experiment_name
        self.run_name = run_name
        self.start_mlflow_run = start_mlflow_run

    def _setup_mlflow(self) -> None:
        """MLflowã®è¨­å®šã‚’è¡Œã†"""
        if not self.use_mlflow:
            return

        # MLflowã®è¨­å®š
        mlflow.set_experiment(self.experiment_name)
        
        # å®Ÿè¡Œåã®è¨­å®š
        if not self.run_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.run_name = f"optuna_tuning_{timestamp}"

        # start_mlflow_runãŒTrueã®å ´åˆã®ã¿æ–°ã—ã„å®Ÿè¡Œã‚’é–‹å§‹
        if self.start_mlflow_run:
            try:
                current_run = mlflow.active_run()
                if current_run is None:
                    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå®Ÿè¡ŒãŒãªã„å ´åˆã®ã¿æ–°ã—ã„å®Ÿè¡Œã‚’é–‹å§‹
                    mlflow.start_run(run_name=self.run_name)
                    self._should_end_run = True
                else:
                    # æ—¢ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå®Ÿè¡ŒãŒã‚ã‚‹å ´åˆã¯çµ‚äº†ã—ãªã„
                    self._should_end_run = False
            except Exception:
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ–°ã—ã„å®Ÿè¡Œã‚’é–‹å§‹
                mlflow.start_run(run_name=self.run_name)
                self._should_end_run = True
        else:
            # æ–°ã—ã„å®Ÿè¡Œã‚’é–‹å§‹ã—ãªã„
            self._should_end_run = False

        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
        mlflow.log_params({
            "outer_splits": self.outer_splits,
            "inner_splits": self.inner_splits,
            "n_trials": self.n_trials,
            "scoring": self.scoring,
            "random_state": self.random_state,
            "tuning_method": "Optuna_TPE",
        })

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ã‚’è¨˜éŒ²
        for param_name, param_range in self.param_ranges.items():
            mlflow.log_param(f"param_range_{param_name}", str(param_range))

    def _log_trial_results(self, trial: optuna.Trial, score: float, fold_idx: int = None) -> None:
        """Optunaã®è©¦è¡Œçµæœã‚’MLflowã«è¨˜éŒ²ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰"""
        # MLflowã«ã¯æœ€çµ‚çµæœã®ã¿ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã€trialçµæœã®è¨˜éŒ²ã¯ç„¡åŠ¹åŒ–
        pass

    def _log_fold_results(self, fold_idx: int, score: float, best_params: dict) -> None:
        """å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®çµæœã‚’MLflowã«è¨˜éŒ²ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰"""
        # MLflowã«ã¯æœ€çµ‚çµæœã®ã¿ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰çµæœã®è¨˜éŒ²ã¯ç„¡åŠ¹åŒ–
        pass

    def _objective(self, trial: optuna.Trial, x_train: pd.DataFrame, y_train: pd.Series, fold_idx: int = None) -> float:
        """Optunaã®ç›®çš„é–¢æ•° - å†…å´CVã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è©•ä¾¡"""
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        params = {}
        for param_name, param_range in self.param_ranges.items():
            if isinstance(param_range, list):
                # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                params[param_name] = trial.suggest_categorical(param_name, param_range)
            elif isinstance(param_range, tuple) and len(param_range) == 2:
                # æ•°å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                if isinstance(param_range[0], int) and isinstance(param_range[1], int):
                    params[param_name] = trial.suggest_int(param_name, param_range[0], param_range[1])
                else:
                    params[param_name] = trial.suggest_float(param_name, param_range[0], param_range[1])
            else:
                raise ValueError(f"Unsupported parameter range format for {param_name}: {param_range}")

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        model = self.estimator_factory()
        model.set_params(**params)

        # å†…å´CVã§è©•ä¾¡
        from sklearn.model_selection import cross_val_score
        scores = cross_val_score(
            model, x_train, y_train, 
            cv=self.inner_splits, 
            scoring=self.scoring,
            n_jobs=1  # Optunaã§ã¯ä¸¦åˆ—å‡¦ç†ã‚’é¿ã‘ã‚‹
        )
        
        score = scores.mean()
        
        return score

    def tune(self, x: pd.DataFrame, y: pd.Series) -> dict[str, Any]:
        """Optunaã‚’ä½¿ç”¨ã—ã¦ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        print(f"ğŸ”„ Optunaãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
        print(f"å¤–å´CVåˆ†å‰²æ•°: {self.outer_splits}")
        print(f"å†…å´CVåˆ†å‰²æ•°: {self.inner_splits}")
        print(f"è©¦è¡Œå›æ•°: {self.n_trials}")
        print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²:")
        for param, range_val in self.param_ranges.items():
            print(f"  - {param}: {range_val}")

        # MLflowã®è¨­å®š
        self._setup_mlflow()

        from sklearn.model_selection import KFold
        
        outer_cv = KFold(n_splits=self.outer_splits, shuffle=True, random_state=self.random_state)
        test_scores: list[float] = []
        best_params_list: list[dict[str, Any]] = []

        for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(x), 1):
            print(f"ğŸ”„ å¤–å´CV {fold_idx}/{self.outer_splits} å›ç›®ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
            x_train, x_test = x.iloc[train_idx], x.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # Optunaã‚¹ã‚¿ãƒ‡ã‚£ã‚’ä½œæˆ
            study = optuna.create_study(
                direction="maximize" if self.scoring in ["r2", "accuracy"] else "minimize",
                sampler=optuna.samplers.TPESampler(seed=self.random_state)
            )

            # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’å®Ÿè¡Œ
            study.optimize(
                lambda trial: self._objective(trial, x_train, y_train, fold_idx),
                n_trials=self.n_trials,
                show_progress_bar=False
            )

            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            best_model = self.estimator_factory()
            best_model.set_params(**study.best_params)
            best_model.fit(x_train, y_train)

            # ãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            score = best_model.score(x_test, y_test)
            test_scores.append(score)
            best_params_list.append(study.best_params)
            
            print(f"   âœ… å¤–å´CV {fold_idx} å›ç›®å®Œäº† - ã‚¹ã‚³ã‚¢: {score:.4f}")
            print(f"   ğŸ¯ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}")

        avg_score = sum(test_scores) / len(test_scores)
        std_score = pd.Series(test_scores).std()
        print(f"ğŸ”§ å¹³å‡ã‚¹ã‚³ã‚¢ ({self.scoring}): {avg_score:.4f}")
        print(f"ğŸ“Š ã‚¹ã‚³ã‚¢ã®æ¨™æº–åå·®: {std_score:.4f}")
        
        # æœ€ã‚‚è‰¯ã„ã‚¹ã‚³ã‚¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™
        best_fold_idx = test_scores.index(max(test_scores))
        best_params = best_params_list[best_fold_idx]
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        print(f"ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
        final_model = self.estimator_factory()
        final_model.set_params(**best_params)
        final_model.fit(x, y)
        print(f"âœ… æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
        
        # MLflowã«æœ€çµ‚çµæœã‚’è¨˜éŒ²
        if self.use_mlflow:
            mlflow.log_metrics({
                "avg_score": avg_score,
                "std_score": std_score,
                "best_fold_score": max(test_scores),
                "worst_fold_score": min(test_scores),
            })
            
            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            for param_name, param_value in best_params.items():
                mlflow.log_param(f"best_{param_name}", param_value)
            
            # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²
            for i, score in enumerate(test_scores, 1):
                mlflow.log_metric(f"fold_{i}_final_score", score)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜
            import joblib
            model_path = "final_model.pkl"
            joblib.dump(final_model, model_path)
            mlflow.log_artifact(model_path, "model")
            
            # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
            import json
            summary = {
                "best_params": best_params,
                "avg_score": avg_score,
                "std_score": std_score,
                "all_scores": test_scores,
                "all_params": best_params_list,
                "tuning_info": {
                    "outer_splits": self.outer_splits,
                    "inner_splits": self.inner_splits,
                    "n_trials": self.n_trials,
                    "scoring": self.scoring,
                    "random_state": self.random_state,
                }
            }
            summary_path = "tuning_summary.json"
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            mlflow.log_artifact(summary_path, "summary")
            
            # MLflowã®å®Ÿè¡Œã‚’çµ‚äº†
            if self._should_end_run:
                mlflow.end_run()
                print(f"ğŸ“Š MLflowã«çµæœã‚’è¨˜éŒ²ã—ã¾ã—ãŸ - å®Ÿé¨“å: {self.experiment_name}, å®Ÿè¡Œå: {self.run_name}")
        
        return {
            "best_params": best_params, 
            "avg_score": avg_score,
            "std_score": std_score,
            "all_scores": test_scores,
            "all_params": best_params_list,
            "final_model": final_model,  # å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸæœ€çµ‚ãƒ¢ãƒ‡ãƒ«
            "mlflow_info": {
                "experiment_name": self.experiment_name,
                "run_name": self.run_name,
                "use_mlflow": self.use_mlflow
            } if self.use_mlflow else None
        }
