import argparse
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import mlflow
import polars as pl

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.models.lightgbm_model import LightGBMRegressor
from src.features.managers.feature_manager import FeatureManager
from src.features.managers.target_manager import TargetManager
from src.pipelines.implementations.tree_pipeline import TreeModelPipeline
from src.data.utils.data_loader import (
    load_data_from_sqlite_polars,
    validate_db_path,
)
from src.data.utils.interactive_selector import (
    get_table_columns,
    interactive_setup,
    validate_table_exists,
)


@dataclass
class Args:
    db_path: str
    table: str = None
    target_column: str = None
    feature_columns: list[str] = None
    save_model: bool = True
    model_save_path: str = "trained_model/lightgbm_model.pkl"
    time_series_split: bool = False
    time_column: str = "date"
    # MLflowé–¢é€£ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    use_mlflow: bool = True
    experiment_name: str = "lightgbm_experiment"
    run_name: str = None
    mlflow_tracking_uri: str = "sqlite:///mlflow.db"
    no_mlflow: bool = False


def _process_datetime_features(data: pl.DataFrame) -> pl.DataFrame:
    """æ—¥æ™‚é–¢é€£ã®ç‰¹å¾´é‡ã‚’å‡¦ç†ã™ã‚‹"""
    processed_data = data.clone()

    # Convert date string to datetime if date column exists
    if "date" in data.columns:
        processed_data = processed_data.with_columns(
            pl.col("date").str.to_datetime("%Y-%m-%d %H:%M:%S").alias("datetime")
        )

        # Day of week: 0=Monday, 1=Tuesday, ..., 6=Sunday
        processed_data = processed_data.with_columns(pl.col("datetime").dt.weekday().alias("dow"))

    return processed_data


def _process_time_features(data: pl.DataFrame) -> pl.DataFrame:
    """æ™‚é–“é–¢é€£ã®ç‰¹å¾´é‡ã‚’å‡¦ç†ã™ã‚‹"""
    processed_data = data.clone()

    # çµ¦æ–™æ—¥ãŒ 25 æ—¥ä»¥é™ãªã®ã§ã€25 æ—¥ä»¥é™ã‚’æœˆæœ«ã¨ã—ã¦ç‰¹å¾´é‡ã«ã™ã‚‹
    if "date_day" in data.columns:
        processed_data = processed_data.with_columns(
            pl.when(pl.col("date_day") >= 25).then(1).otherwise(0).alias("is_month_end")
        )

    # é€±æœ«
    if "dow" in processed_data.columns:
        processed_data = processed_data.with_columns(
            pl.when(pl.col("dow") >= 4).then(1).otherwise(0).alias("is_weekend")
        )

    # ãƒ©ãƒ³ãƒã‚¿ã‚¤ãƒ ãƒ»ãƒ‡ã‚£ãƒŠãƒ¼ã‚¿ã‚¤ãƒ 
    if "time" in data.columns:
        processed_data = processed_data.with_columns(
            pl.when(pl.col("time").is_in([11, 12, 13])).then(1).otherwise(0).alias("is_lunch")
        )
        processed_data = processed_data.with_columns(
            pl.when(pl.col("time") >= 18).then(1).otherwise(0).alias("is_dinner")
        )

    return processed_data


def _convert_string_columns(data: pl.DataFrame, available_columns: list[str]) -> tuple[pl.DataFrame, list[str]]:
    """æ–‡å­—åˆ—å‹ã®åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ã™ã‚‹"""
    processed_data = data.clone()
    updated_columns = available_columns.copy()

    print("\nğŸ” ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª:")
    for col in processed_data.columns:
        dtype = processed_data[col].dtype
        print(f"  {col}: {dtype}")

        # æ–‡å­—åˆ—å‹ã®å ´åˆã¯æ•°å€¤ã«å¤‰æ›ã‚’è©¦è¡Œ
        if dtype == pl.Utf8:
            print(f"    âš ï¸  æ–‡å­—åˆ—å‹ã®åˆ— '{col}' ã‚’æ•°å€¤ã«å¤‰æ›ã—ã¾ã™")
            try:
                # ç©ºæ–‡å­—åˆ—ã‚’NaNã«å¤‰æ›ã—ã¦ã‹ã‚‰æ•°å€¤ã«å¤‰æ›
                processed_data = processed_data.with_columns(
                    pl.col(col).str.replace("", "null").cast(pl.Float64, strict=False)
                )
                print("    âœ… å¤‰æ›æˆåŠŸ")
            except Exception as e:
                print(f"    âŒ å¤‰æ›å¤±æ•—: {e}")
                # å¤‰æ›ã§ããªã„å ´åˆã¯é™¤å¤–
                if col in updated_columns:
                    updated_columns.remove(col)
                    print(f"    ğŸ—‘ï¸  åˆ— '{col}' ã‚’é™¤å¤–ã—ã¾ã™")

    return processed_data, updated_columns


def _handle_missing_values(data: pl.DataFrame) -> pl.DataFrame:
    """æ¬ æå€¤ã‚’å‡¦ç†ã™ã‚‹"""
    processed_data = data.clone()

    print("\nğŸ”§ æ¬ æå€¤ã®å‡¦ç†:")
    for col in processed_data.columns:
        null_count = processed_data[col].null_count()
        if null_count > 0:
            print(f"  {col}: {null_count}å€‹ã®æ¬ æå€¤ã‚’0ã§è£œå®Œ")
            processed_data = processed_data.with_columns(pl.col(col).fill_null(0))

    return processed_data


def feature_engineering(
    data: pl.DataFrame, feature_columns: list[str], keep_date_for_split: bool = False
) -> pl.DataFrame:
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    # æ—¥æ™‚é–¢é€£ã®ç‰¹å¾´é‡ã‚’å‡¦ç†
    processed_data = _process_datetime_features(data)

    # æ™‚é–“é–¢é€£ã®ç‰¹å¾´é‡ã‚’å‡¦ç†
    processed_data = _process_time_features(processed_data)

    # æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡åˆ—ã®ã¿ã‚’é¸æŠ
    available_columns = [col for col in feature_columns if col in processed_data.columns]

    # æ™‚ç³»åˆ—åˆ†å‰²ãŒå¿…è¦ãªå ´åˆã¯dateåˆ—ã‚’ä¿æŒ(datetimeåˆ—ã«å¤‰æ›ã•ã‚Œã¦ã‚‚å…ƒã®dateåˆ—ã‚’ä¿æŒ)
    if keep_date_for_split and "date" in data.columns and "date" not in available_columns:
        available_columns.append("date")

    processed_data = processed_data.select(available_columns)

    # æ–‡å­—åˆ—å‹ã®åˆ—ã‚’æ•°å€¤ã«å¤‰æ›
    processed_data, available_columns = _convert_string_columns(processed_data, available_columns)

    # æ¬ æå€¤ã®å‡¦ç†
    processed_data = _handle_missing_values(processed_data)

    # æœ€çµ‚çš„ãªåˆ—é¸æŠ
    final_columns = [col for col in available_columns if col in processed_data.columns]
    processed_data = processed_data.select(final_columns)

    print("\nğŸ“Š ç‰¹å¾´é‡ã®ç¢ºèª:")
    print(processed_data.head())
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡åˆ—: {final_columns}")
    print("æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿å‹:")
    for col in processed_data.columns:
        print(f"  {col}: {processed_data[col].dtype}")

    return processed_data


def parse_and_validate_args() -> Args:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã€å¿…è¦ã«å¿œã˜ã¦å¯¾è©±çš„è¨­å®šã‚’è¡Œã†"""
    parser = argparse.ArgumentParser(
        description="Interactive ML Model Training with SQLite and MLflow Integration",
        epilog="""
Examples:
  # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ï¼ˆMLflowæœ‰åŠ¹ï¼‰
  python src/scripts/07_concise_draftmodel.py

  # MLflowã‚’ç„¡åŠ¹ã«ã™ã‚‹å ´åˆ
  python src/scripts/07_concise_draftmodel.py --no-mlflow

  # ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“åã‚’æŒ‡å®š
  python src/scripts/07_concise_draftmodel.py --experiment-name "my_experiment"

  # MLflow UIã§çµæœã‚’ç¢ºèª
  python -m mlflow ui
  # ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:5000 ã«ã‚¢ã‚¯ã‚»ã‚¹
        """
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/database.sqlite",
        help="SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/database.sqlite)",
    )
    parser.add_argument(
        "--table",
        type=str,
        help="ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--target-column",
        type=str,
        help="ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--feature-columns",
        type=str,
        nargs="+",
        help="ç‰¹å¾´é‡åˆ—å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ãªã„",
    )
    parser.add_argument(
        "--model-save-path",
        type=str,
        default="trained_model/lightgbm_model.pkl",
        help="ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trained_model/lightgbm_model.pkl)",
    )
    parser.add_argument(
        "--time-series-split",
        action="store_true",
        help="æ™‚ç³»åˆ—åˆ†å‰²ã‚’å®Ÿè¡Œã™ã‚‹",
    )
    parser.add_argument(
        "--time-column",
        type=str,
        default="date",
        help="æ™‚ç³»åˆ—åˆ†å‰²ã«ä½¿ç”¨ã™ã‚‹æ—¥ä»˜åˆ—å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: date)",
    )
    parser.add_argument(
        "--use-mlflow",
        action="store_true",
        help="MLflowã‚’ä½¿ç”¨ã™ã‚‹",
    )
    parser.add_argument(
        "--no-mlflow",
        action="store_true",
        help="MLflowã‚’ä½¿ç”¨ã—ãªã„",
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        default="lightgbm_experiment",
        help="MLflowã®å®Ÿé¨“å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: lightgbm_experiment)",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        help="MLflowã®å®Ÿè¡Œå (æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ)",
    )
    parser.add_argument(
        "--mlflow-tracking-uri",
        type=str,
        default="sqlite:///mlflow.db",
        help="MLflowã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: sqlite:///mlflow.db)",
    )
    args = parser.parse_args(namespace=Args)

    # æ™‚ç³»åˆ—åˆ†å‰²ã®è¨­å®šã‚’Argsã«åæ˜ 
    args.time_series_split = args.time_series_split
    args.time_column = args.time_column
    # MLflowé–¢é€£ã®è¨­å®šã‚’Argsã«åæ˜ 
    args.use_mlflow = args.use_mlflow
    args.experiment_name = args.experiment_name
    args.run_name = args.run_name
    args.mlflow_tracking_uri = args.mlflow_tracking_uri
    
    # --no-mlflowã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯MLflowã‚’ç„¡åŠ¹åŒ–
    if args.no_mlflow:
        args.use_mlflow = False

    print("ğŸš€ Interactive ML Model Training with SQLite")
    print("=" * 60)
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.db_path}")

    db_path = Path(args.db_path)
    if not validate_db_path(db_path):
        sys.exit(1)

    # å¯¾è©±çš„è¨­å®š(ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ)
    if not args.table or not args.target_column or not args.feature_columns:
        try:
            table_name, target_column, feature_columns = interactive_setup(db_path)
            args.table = table_name
            args.target_column = target_column
            args.feature_columns = feature_columns
        except ValueError as e:
            print(f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)
    else:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå ´åˆã®æ¤œè¨¼
        if not validate_table_exists(db_path, args.table):
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« '{args.table}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            sys.exit(1)

        all_columns = [col[1] for col in get_table_columns(db_path, args.table)]
        if args.target_column not in all_columns:
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ— '{args.target_column}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            sys.exit(1)

        missing_columns = [col for col in args.feature_columns if col not in all_columns]
        if missing_columns:
            print(f"âŒ ä»¥ä¸‹ã®ç‰¹å¾´é‡åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}")
            sys.exit(1)

    return args


def train_model(args: Args, data: pl.DataFrame):
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œ"""
    # MLflowã®è¨­å®š
    if args.use_mlflow:
        print(f"\nğŸ”§ MLflowè¨­å®š:")
        print(f"  ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI: {args.mlflow_tracking_uri}")
        print(f"  å®Ÿé¨“å: {args.experiment_name}")
        
        # MLflowã®è¨­å®š
        mlflow.set_tracking_uri(args.mlflow_tracking_uri)
        mlflow.set_experiment(args.experiment_name)
        
        # å®Ÿè¡Œåã®è¨­å®š
        if not args.run_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            args.run_name = f"lightgbm_{args.table}_{timestamp}"
        
        print(f"  å®Ÿè¡Œå: {args.run_name}")
        
        # MLflowã®å®Ÿè¡Œã‚’é–‹å§‹
        mlflow.start_run(run_name=args.run_name)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
        mlflow.log_param("table_name", args.table)
        mlflow.log_param("target_column", args.target_column)
        mlflow.log_param("feature_count", len(args.feature_columns))
        mlflow.log_param("data_size", len(data))
        mlflow.log_param("time_series_split", args.time_series_split)
        if args.time_series_split:
            mlflow.log_param("time_column", args.time_column)
        mlflow.log_param("model_type", "LightGBM")
        mlflow.log_param("num_boost_round", 200)
        mlflow.log_param("early_stopping_rounds", 10)
        mlflow.log_param("test_size", 0.1)
        mlflow.log_param("random_state", 42)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    target_data = data[args.target_column]
    target_manager = TargetManager(target_data=target_data)

    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("\nğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    engineered_data = feature_engineering(data, args.feature_columns, args.time_series_split)
    feature_manager = FeatureManager(initial_features=engineered_data)

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º
    print(f"\n{feature_manager}")
    print(f"\n{target_manager}")

    # LightGBMãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    model = LightGBMRegressor(
        num_boost_round=200,
        early_stopping_rounds=10,
        params={"objective": "regression", "metric": "rmse", "verbose": 1, "seed": 42},
        verbose_eval=True,
    )

    # è‡ªå‹•çš„ã«å­¦ç¿’æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
    print("\nğŸ“ è‡ªå‹•çš„ã«å­¦ç¿’æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ ...")
    model.add_comment(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {args.table} (SQLite)")
    model.add_comment(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—: {args.target_column}")
    model.add_comment(f"ç‰¹å¾´é‡æ•°: {len(args.feature_columns)}")
    model.add_comment(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data)} è¡Œ")
    model.add_comment(f"åˆ†å‰²æ–¹æ³•: {'æ™‚ç³»åˆ—åˆ†å‰²' if args.time_series_split else 'ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²'}")
    if args.time_series_split:
        model.add_comment(f"æ™‚ç³»åˆ—åˆ—: {args.time_column}")
    model.add_comment("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°: æ—¥æ™‚ç‰¹å¾´é‡ã€æ™‚é–“ç‰¹å¾´é‡ã€æ–‡å­—åˆ—å¤‰æ›ã€æ¬ æå€¤è£œå®Œ")
    model.add_comment("ãƒ¢ãƒ‡ãƒ«: LightGBM (å›å¸°)")
    model.add_comment("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š (num_boost_round=200, early_stopping_rounds=10)")

    # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¡ãƒ³ãƒˆã®å…¥åŠ›
    print("\nğŸ“ ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¡ãƒ³ãƒˆã®å…¥åŠ›")
    print("=" * 40)
    print("å­¦ç¿’æ™‚ã®æƒ…å ±ã¨ã—ã¦ä¿å­˜ã™ã‚‹è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    print("ï¼ˆç©ºè¡Œã§å…¥åŠ›ã‚’çµ‚äº†ã—ã¾ã™ï¼‰")

    skip_comments = input("è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
    if skip_comments in ["y", "yes"]:
        print("âš ï¸  è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆå…¥åŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
        comments = []
    else:
        comments = []
        comment_count = 1
        while True:
            comment = input(f"è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆ {comment_count}: ").strip()
            if not comment:
                break
            comments.append(comment)
            comment_count += 1

    # è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ 
    for comment in comments:
        model.add_comment(comment)

    if comments:
        print(f"\nâœ… {len(comments)}å€‹ã®è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸ:")
        for i, comment in enumerate(comments, 1):
            print(f"  {i}. {comment}")
    else:
        print("\nâš ï¸  è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã¯è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    # å…¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º
    print("\nğŸ“‹ ä¿å­˜ã•ã‚Œã‚‹å…¨ã‚³ãƒ¡ãƒ³ãƒˆ:")
    all_comments = model.get_comments()
    for i, comment in enumerate(all_comments, 1):
        print(f"  {i}. {comment}")

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    pipeline = TreeModelPipeline(model=model, feature_manager=feature_manager, target_manager=target_manager)

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    print("\nğŸ”„ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    if args.time_series_split:
        print(f"  æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ (æ™‚ç³»åˆ—åˆ—: {args.time_column})")
    else:
        print("  ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã‚’ä½¿ç”¨")

    results = pipeline.train(
        test_size=0.1, random_state=42, time_series_split=args.time_series_split, time_column=args.time_column
    )

    # çµæœè¡¨ç¤º
    print("\nğŸ“ˆ å­¦ç¿’çµæœ:")
    print(f"{results}")

    # MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
    if args.use_mlflow:
        print("\nğŸ“Š MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ä¸­...")
        # çµæœã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
        if hasattr(results, 'train_rmse'):
            mlflow.log_metric("train_rmse", results.train_rmse)
        if hasattr(results, 'test_rmse'):
            mlflow.log_metric("test_rmse", results.test_rmse)
        if hasattr(results, 'train_r2'):
            mlflow.log_metric("train_r2", results.train_r2)
        if hasattr(results, 'test_r2'):
            mlflow.log_metric("test_r2", results.test_r2)
        if hasattr(results, 'train_mae'):
            mlflow.log_metric("train_mae", results.train_mae)
        if hasattr(results, 'test_mae'):
            mlflow.log_metric("test_mae", results.test_mae)
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¨˜éŒ²
        importance = model.get_feature_importance()
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, imp) in enumerate(sorted_importance[:10]):
            mlflow.log_metric(f"feature_importance_{feature}", imp)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¿ã‚°ã¨ã—ã¦è¨˜éŒ²
        for i, comment in enumerate(all_comments):
            mlflow.set_tag(f"comment_{i+1}", comment)

    # å­¦ç¿’æƒ…å ±ã‚’è¡¨ç¤º
    print("\nğŸ“Š å­¦ç¿’æ™‚ã®æƒ…å ±:")
    model.print_training_info()

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¡¨ç¤º
    importance = model.get_feature_importance()
    print("\nğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦ (ãƒˆãƒƒãƒ—10):")
    sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
    for feature, imp in sorted_importance[:10]:
        print(f"  {feature}: {imp:.4f}")

    return pipeline


def save_model(args: Args, pipeline):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
    if not args.no_save:
        try:
            save_path = Path(args.model_save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’ä¿å­˜
            pipeline.save_model(save_path)
            print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")

            # MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜
            if args.use_mlflow:
                print("\nğŸ“¦ MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜ä¸­...")
                mlflow.log_artifact(str(save_path), "model")
                
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
                config_path = save_path.with_suffix(".json")
                import json

                config = {
                    "table_name": args.table,
                    "target_column": args.target_column,
                    "feature_columns": args.feature_columns,
                    "model_type": "LightGBM",
                    "save_timestamp": str(datetime.now()),
                    "comments": pipeline.get_model().get_comments() if hasattr(pipeline.get_model(), "get_comments") else [],
                    "training_info": pipeline.get_model().get_training_info() if hasattr(pipeline.get_model(), "get_training_info") else {},
                }
                with open(config_path, "w", encoding="utf-8") as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                
                mlflow.log_artifact(str(config_path), "config")
                print(f"ğŸ“‹ è¨­å®šæƒ…å ±ã‚’MLflowã«ä¿å­˜ã—ã¾ã—ãŸ: {config_path}")

            # ä¿å­˜ã•ã‚ŒãŸå­¦ç¿’æƒ…å ±ã‚’ç¢ºèª
            model = pipeline.get_model()
            if hasattr(model, "get_training_info"):
                training_info = model.get_training_info()
                print("ğŸ“Š ä¿å­˜ã•ã‚ŒãŸå­¦ç¿’æƒ…å ±:")
                print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {training_info.get('data_size', 'N/A')}")
                print(f"  ç‰¹å¾´é‡æ•°: {training_info.get('feature_count', 'N/A')}")
                print(f"  å­¦ç¿’æ—¥æ™‚: {training_info.get('training_timestamp', 'N/A')}")

            if hasattr(model, "get_comments"):
                comments = model.get_comments()
                if comments:
                    print(f"ğŸ“ ä¿å­˜ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆ ({len(comments)}å€‹):")
                    # è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆã¨æ‰‹å‹•ã‚³ãƒ¡ãƒ³ãƒˆã‚’åŒºåˆ¥ã—ã¦è¡¨ç¤º
                    auto_comments = []
                    manual_comments = []

                    for comment in comments:
                        if any(
                            keyword in comment
                            for keyword in [
                                "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:",
                                "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—:",
                                "ç‰¹å¾´é‡æ•°:",
                                "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:",
                                "åˆ†å‰²æ–¹æ³•:",
                                "æ™‚ç³»åˆ—åˆ—:",
                                "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:",
                                "ãƒ¢ãƒ‡ãƒ«:",
                                "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:",
                            ]
                        ):
                            auto_comments.append(comment)
                        else:
                            manual_comments.append(comment)

                    if auto_comments:
                        print("  ğŸ”§ è‡ªå‹•ç”Ÿæˆã‚³ãƒ¡ãƒ³ãƒˆ:")
                        for i, comment in enumerate(auto_comments, 1):
                            print(f"    {i}. {comment}")

                    if manual_comments:
                        print("  âœï¸  æ‰‹å‹•å…¥åŠ›ã‚³ãƒ¡ãƒ³ãƒˆ:")
                        for i, comment in enumerate(manual_comments, 1):
                            print(f"    {i}. {comment}")
                else:
                    print("ğŸ“ ä¿å­˜ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆ: ãªã—")

            # è¨­å®šæƒ…å ±ã‚‚ä¿å­˜ï¼ˆMLflowã‚’ä½¿ç”¨ã—ãªã„å ´åˆï¼‰
            if not args.use_mlflow:
                config_path = save_path.with_suffix(".json")
                import json

                config = {
                    "table_name": args.table,
                    "target_column": args.target_column,
                    "feature_columns": args.feature_columns,
                    "model_type": "LightGBM",
                    "save_timestamp": str(datetime.now()),
                    "comments": model.get_comments() if hasattr(model, "get_comments") else [],
                    "training_info": model.get_training_info() if hasattr(model, "get_training_info") else {},
                }
                with open(config_path, "w", encoding="utf-8") as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“‹ è¨­å®šæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {config_path}")

        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("\nâš ï¸  ãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ (--no-save ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    
    # MLflowã®å®Ÿè¡Œã‚’çµ‚äº†
    if args.use_mlflow:
        mlflow.end_run()
        print("\nâœ… MLflowã®å®Ÿè¡Œã‚’çµ‚äº†ã—ã¾ã—ãŸ")


def main():
    # å¼•æ•°ã®è§£æã¨æ¤œè¨¼
    args = parse_and_validate_args()

    # SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    try:
        print("\nğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        data = load_data_from_sqlite_polars(args.db_path, args.table)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {data.shape}")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±è¡¨ç¤º
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±:")
    print(f"  å½¢çŠ¶: {data.shape}")
    print(f"  åˆ—å: {data.columns}")
    print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—: {args.target_column}")
    print(f"  ç‰¹å¾´é‡åˆ—æ•°: {len(args.feature_columns)}")

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    pipeline = train_model(args, data)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    save_model(args, pipeline)

    # MLflowã®ä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
    if args.use_mlflow:
        print(f"\nğŸ“Š MLflowæƒ…å ±:")
        print(f"  å®Ÿé¨“å: {args.experiment_name}")
        print(f"  å®Ÿè¡Œå: {args.run_name}")
        print(f"  ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI: {args.mlflow_tracking_uri}")
        print("  MLflow UIã§çµæœã‚’ç¢ºèªã§ãã¾ã™:")
        print("    python -m mlflow ui")
        print("    ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:5000 ã«ã‚¢ã‚¯ã‚»ã‚¹")
    else:
        print(f"\nâš ï¸  MLflowã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ (--no-mlflow ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¾ãŸã¯ --use-mlflow ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“)")

    print("\nâœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")


if __name__ == "__main__":
    main()
