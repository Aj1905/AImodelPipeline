import argparse
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import mlflow
import polars as pl

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.data.utils.data_loader import (
    load_data_from_sqlite_polars,
    validate_db_path,
)
from src.data.utils.interactive_selector import (
    get_table_columns,
    interactive_setup,
    validate_table_exists,
)
from src.features.managers.feature_manager import FeatureManager
from src.features.managers.target_manager import TargetManager
from src.models.implementations.lightgbm_model import LightGBMRegressor
from src.pipelines.implementations.tree_pipeline import TreeModelPipeline


@dataclass
class Args:
    db_path: str
    table: str = None
    target_column: str = None
    feature_columns: list[str] = None
    save_model: bool = True
    model_save_path: str = "trained_model/lightgbm_model.pkl"
    time_series_split: bool = False
    time_column: str = "date"
    # MLflowé–¢é€£ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    use_mlflow: bool = True
    experiment_name: str = "lightgbm_experiment"
    run_name: str = None
    mlflow_tracking_uri: str = "sqlite:///mlflow.db"
    no_mlflow: bool = False

    def __post_init__(self):
        # feature_columnsãŒNoneã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã«åˆæœŸåŒ–
        if self.feature_columns is None:
            self.feature_columns = []


def parse_and_validate_args() -> Args:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã€å¿…è¦ã«å¿œã˜ã¦å¯¾è©±çš„è¨­å®šã‚’è¡Œã†"""
    parser = argparse.ArgumentParser(
        description="Interactive ML Model Training with SQLite and MLflow Integration",
        epilog="""
Examples:
  # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•(MLflowæœ‰åŠ¹)
  python src/scripts/07_concise_draftmodel.py

  # MLflowã‚’ç„¡åŠ¹ã«ã™ã‚‹å ´åˆ
  python src/scripts/07_concise_draftmodel.py --no-mlflow

  # ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“åã‚’æŒ‡å®š
  python src/scripts/07_concise_draftmodel.py --experiment-name "my_experiment"

  # MLflow UIã§çµæœã‚’ç¢ºèª
  python -m mlflow ui
  # ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:5000 ã«ã‚¢ã‚¯ã‚»ã‚¹
        """
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/database.sqlite",
        help="SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/database.sqlite)",
    )
    parser.add_argument(
        "--table",
        type=str,
        help="ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--target-column",
        type=str,
        help="ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--feature-columns",
        type=str,
        nargs="+",
        help="ç‰¹å¾´é‡åˆ—å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ãªã„",
    )
    parser.add_argument(
        "--model-save-path",
        type=str,
        default="trained_model/lightgbm_model.pkl",
        help="ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trained_model/lightgbm_model.pkl)",
    )
    parser.add_argument(
        "--time-series-split",
        action="store_true",
        help="æ™‚ç³»åˆ—åˆ†å‰²ã‚’å®Ÿè¡Œã™ã‚‹",
    )
    parser.add_argument(
        "--time-column",
        type=str,
        default="date",
        help="æ™‚ç³»åˆ—åˆ†å‰²ã«ä½¿ç”¨ã™ã‚‹æ—¥ä»˜åˆ—å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: date)",
    )
    parser.add_argument(
        "--use-mlflow",
        action="store_true",
        help="MLflowã‚’ä½¿ç”¨ã™ã‚‹",
    )
    parser.add_argument(
        "--no-mlflow",
        action="store_true",
        help="MLflowã‚’ä½¿ç”¨ã—ãªã„",
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        default="lightgbm_experiment",
        help="MLflowã®å®Ÿé¨“å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: lightgbm_experiment)",
    )
    parser.add_argument(
        "--run-name",
        type=str,
        help="MLflowã®å®Ÿè¡Œå (æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ)",
    )
    parser.add_argument(
        "--mlflow-tracking-uri",
        type=str,
        default="sqlite:///mlflow.db",
        help="MLflowã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: sqlite:///mlflow.db)",
    )
    args = parser.parse_args(namespace=Args)

    # æ™‚ç³»åˆ—åˆ†å‰²ã®è¨­å®šã‚’Argsã«åæ˜ 
    args.time_series_split = args.time_series_split
    args.time_column = args.time_column
    # MLflowé–¢é€£ã®è¨­å®šã‚’Argsã«åæ˜ 
    args.use_mlflow = args.use_mlflow
    args.experiment_name = args.experiment_name
    args.run_name = args.run_name
    args.mlflow_tracking_uri = args.mlflow_tracking_uri

    # --no-mlflowã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯MLflowã‚’ç„¡åŠ¹åŒ–
    if args.no_mlflow:
        args.use_mlflow = False

    print("ğŸš€ Interactive ML Model Training with SQLite")
    print("=" * 60)
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.db_path}")

    db_path = Path(args.db_path)
    if not validate_db_path(db_path):
        sys.exit(1)

    # å¯¾è©±çš„è¨­å®š(ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ)
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’æ­£ç¢ºã«åˆ¤å®š
    args_specified = (
        args.table is not None and
        args.target_column is not None and
        len(args.feature_columns) > 0
    )

    if not args_specified:
        print("ğŸ”§ ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒä¸å®Œå…¨ãªãŸã‚ã€å¯¾è©±çš„è¨­å®šã‚’é–‹å§‹ã—ã¾ã™...")
        try:
            table_name, target_column, feature_columns = interactive_setup(db_path)
            args.table = table_name
            args.target_column = target_column
            args.feature_columns = feature_columns
        except ValueError as e:
            print(f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            sys.exit(1)
    else:
        print("âœ… ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå ´åˆã®æ¤œè¨¼
        if not validate_table_exists(db_path, args.table):
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« '{args.table}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            sys.exit(1)

        all_columns = [col[1] for col in get_table_columns(db_path, args.table)]
        if args.target_column not in all_columns:
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ— '{args.target_column}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            sys.exit(1)

        missing_columns = [
            col for col in args.feature_columns if col not in all_columns
        ]
        if missing_columns:
            print(f"âŒ ä»¥ä¸‹ã®ç‰¹å¾´é‡åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}")
            sys.exit(1)

    return args


def setup_mlflow(args: Args, data: pl.DataFrame) -> None:
    """MLflowã®è¨­å®šã‚’è¡Œã†"""
    if not args.use_mlflow:
        return

    print("\nğŸ”§ MLflowè¨­å®š:")
    print(f"  ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°URI: {args.mlflow_tracking_uri}")
    print(f"  å®Ÿé¨“å: {args.experiment_name}")

    # MLflowã®è¨­å®š
    mlflow.set_tracking_uri(args.mlflow_tracking_uri)
    mlflow.set_experiment(args.experiment_name)

    # å®Ÿè¡Œåã®è¨­å®š
    if not args.run_name:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.run_name = f"lightgbm_{args.table}_{timestamp}"

    print(f"  å®Ÿè¡Œå: {args.run_name}")

    # MLflowã®å®Ÿè¡Œã‚’é–‹å§‹
    mlflow.start_run(run_name=args.run_name)

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
    mlflow.log_param("table_name", args.table)
    mlflow.log_param("target_column", args.target_column)
    mlflow.log_param("feature_count", len(args.feature_columns))
    mlflow.log_param("data_size", len(data))
    mlflow.log_param("time_series_split", args.time_series_split)
    if args.time_series_split:
        mlflow.log_param("time_column", args.time_column)
    mlflow.log_param("model_type", "LightGBM")
    mlflow.log_param("num_boost_round", 200)
    mlflow.log_param("early_stopping_rounds", 10)
    mlflow.log_param("test_size", 0.1)
    mlflow.log_param("random_state", 42)


def setup_model_and_comments(
    args: Args, data: pl.DataFrame
) -> tuple[LightGBMRegressor, list[str]]:
    """ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã¨ã‚³ãƒ¡ãƒ³ãƒˆã®è¿½åŠ ã‚’è¡Œã†"""
    # LightGBMãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
    model = LightGBMRegressor(
        num_boost_round=200,
        early_stopping_rounds=10,
        params={"objective": "regression", "metric": "rmse", "verbose": 1, "seed": 42},
        verbose_eval=True,
    )

    # è‡ªå‹•çš„ã«å­¦ç¿’æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
    print("\nğŸ“ è‡ªå‹•çš„ã«å­¦ç¿’æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ ...")
    model.add_comment(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {args.table} (SQLite)")
    model.add_comment(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—: {args.target_column}")
    model.add_comment(f"ç‰¹å¾´é‡æ•°: {len(args.feature_columns)}")
    model.add_comment(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data)} è¡Œ")
    split_method = "æ™‚ç³»åˆ—åˆ†å‰²" if args.time_series_split else "ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²"
    model.add_comment(f"åˆ†å‰²æ–¹æ³•: {split_method}")
    if args.time_series_split:
        model.add_comment(f"æ™‚ç³»åˆ—åˆ—: {args.time_column}")
    model.add_comment("ãƒ¢ãƒ‡ãƒ«: LightGBM (å›å¸°)")
    model.add_comment(
        "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š "
        "(num_boost_round=200, early_stopping_rounds=10)"
    )

    # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¡ãƒ³ãƒˆã®å…¥åŠ›
    comments = get_custom_comments()

    # è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ 
    for comment in comments:
        model.add_comment(comment)

    if comments:
        print(f"\nâœ… {len(comments)}å€‹ã®è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸ:")
        for i, comment in enumerate(comments, 1):
            print(f"  {i}. {comment}")
    else:
        print("\nâš ï¸  è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã¯è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    # å…¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º
    print("\nğŸ“‹ ä¿å­˜ã•ã‚Œã‚‹å…¨ã‚³ãƒ¡ãƒ³ãƒˆ:")
    all_comments = model.get_comments()
    for i, comment in enumerate(all_comments, 1):
        print(f"  {i}. {comment}")

    return model, all_comments


def get_custom_comments() -> list[str]:
    """ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹"""
    print("\nğŸ“ ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¡ãƒ³ãƒˆã®å…¥åŠ›")
    print("=" * 40)
    print("å­¦ç¿’æ™‚ã®æƒ…å ±ã¨ã—ã¦ä¿å­˜ã™ã‚‹è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    print("(ç©ºè¡Œã§å…¥åŠ›ã‚’çµ‚äº†ã—ã¾ã™)")

    skip_comments = input("è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã‹? (y/N): ").strip().lower()
    if skip_comments in ["y", "yes"]:
        print("âš ï¸  è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆå…¥åŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
        return []

    comments = []
    comment_count = 1
    while True:
        comment = input(f"è¿½åŠ ã‚³ãƒ¡ãƒ³ãƒˆ {comment_count}: ").strip()
        if not comment:
            break
        comments.append(comment)
        comment_count += 1

    return comments


def log_mlflow_metrics(
    args: Args, model: LightGBMRegressor, results, all_comments: list[str], cv_results
) -> None:
    """MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹"""
    if not args.use_mlflow:
        return

    print("\nğŸ“Š MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ä¸­...")

    # TrainingResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ­£ã—ãè©•ä¾¡æŒ‡æ¨™ã‚’å–å¾—
    if hasattr(results, 'train_metrics') and hasattr(results, 'validation_metrics'):
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡æŒ‡æ¨™
        mlflow.log_metric("train_mse", results.train_metrics.mse)
        mlflow.log_metric("train_rmse", results.train_metrics.rmse)
        mlflow.log_metric("train_mae", results.train_metrics.mae)
        mlflow.log_metric("train_r2", results.train_metrics.r2)

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡æŒ‡æ¨™
        mlflow.log_metric("validation_mse", results.validation_metrics.mse)
        mlflow.log_metric("validation_rmse", results.validation_metrics.rmse)
        mlflow.log_metric("validation_mae", results.validation_metrics.mae)
        mlflow.log_metric("validation_r2", results.validation_metrics.r2)

        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæƒ…å ±
        mlflow.log_metric("train_size", results.train_size)
        mlflow.log_metric("test_size", results.test_size)
        mlflow.log_metric("feature_count", results.feature_count)

        print("âœ… è©•ä¾¡æŒ‡æ¨™ã‚’MLflowã«è¨˜éŒ²ã—ã¾ã—ãŸ")
    else:
        print("âš ï¸  TrainingResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰è©•ä¾¡æŒ‡æ¨™ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¨˜éŒ²
    importance = model.get_feature_importance()
    sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
    for _i, (feature, imp) in enumerate(sorted_importance[:10]):
        mlflow.log_metric(f"feature_importance_{feature}", imp)

    # ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚¿ã‚°ã¨ã—ã¦è¨˜éŒ²
    for i, comment in enumerate(all_comments):
        mlflow.set_tag(f"comment_{i+1}", comment)

    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¨˜éŒ²
    mlflow.log_metric("cv_mse", cv_results.mean_metrics.mse)
    mlflow.log_metric("cv_rmse", cv_results.mean_metrics.rmse)
    mlflow.log_metric("cv_mae", cv_results.mean_metrics.mae)
    mlflow.log_metric("cv_r2", cv_results.mean_metrics.r2)


def train_model(args: Args, data: pl.DataFrame):
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿè¡Œ"""
    # MLflowã®è¨­å®š
    setup_mlflow(args, data)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    target_data = data[args.target_column]
    target_manager = TargetManager(target_data=target_data)

    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™(ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨)
    feature_data = data.select(args.feature_columns)
    feature_manager = FeatureManager(initial_features=feature_data)

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º
    print(f"\n{feature_manager}")
    print(f"\n{target_manager}")

    # ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã¨ã‚³ãƒ¡ãƒ³ãƒˆã®è¿½åŠ 
    model, all_comments = setup_model_and_comments(args, data)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    pipeline = TreeModelPipeline(
        model=model, feature_manager=feature_manager, target_manager=target_manager
    )

    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    print("\nğŸ”„ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
    cv_results = pipeline.cross_validate(cv_folds=5, random_state=42)
    print("\nğŸ“Š ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
    print(f"{cv_results}")

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    print("\nğŸ”„ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    if args.time_series_split:
        print(f"  æ™‚ç³»åˆ—åˆ†å‰²ã‚’ä½¿ç”¨ (æ™‚ç³»åˆ—åˆ—: {args.time_column})")
    else:
        print("  ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã‚’ä½¿ç”¨")

    results = pipeline.train(
        test_size=0.1,
        random_state=42,
        time_series_split=args.time_series_split,
        time_column=args.time_column,
    )

    # çµæœè¡¨ç¤º
    print(f"\nğŸ“ˆ\n{results}")

    # MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
    log_mlflow_metrics(args, model, results, all_comments, cv_results)

    model.print_training_info()

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¡¨ç¤º
    importance = model.get_feature_importance()
    print("\nğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦ (ãƒˆãƒƒãƒ—10):")
    sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
    for feature, imp in sorted_importance[:10]:
        print(f"  {feature}: {imp:.4f}")

    return pipeline


def save_model_to_mlflow(args: Args, save_path: Path, pipeline) -> None:
    """MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹"""
    if not args.use_mlflow:
        return

    print("\nğŸ“¦ MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜ä¸­...")
    mlflow.log_artifact(str(save_path), "model")

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
    config_path = save_path.with_suffix(".json")
    import json

    config = {
        "table_name": args.table,
        "target_column": args.target_column,
        "feature_columns": args.feature_columns,
        "model_type": "LightGBM",
        "save_timestamp": str(datetime.now()),
        "comments": (
            pipeline.get_model().get_comments()
            if hasattr(pipeline.get_model(), "get_comments")
            else []
        ),
        "training_info": (
            pipeline.get_model().get_training_info()
            if hasattr(pipeline.get_model(), "get_training_info")
            else {}
        ),
    }
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    mlflow.log_artifact(str(config_path), "config")
    print(f"ğŸ“‹ è¨­å®šæƒ…å ±ã‚’MLflowã«ä¿å­˜ã—ã¾ã—ãŸ: {config_path}")


def save_config_file(args: Args, save_path: Path, model) -> None:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹"""
    if args.use_mlflow:
        return

    config_path = save_path.with_suffix(".json")
    import json

    config = {
        "table_name": args.table,
        "target_column": args.target_column,
        "feature_columns": args.feature_columns,
        "model_type": "LightGBM",
        "save_timestamp": str(datetime.now()),
        "comments": (
            model.get_comments() if hasattr(model, "get_comments") else []
        ),
        "training_info": (
            model.get_training_info() if hasattr(model, "get_training_info") else {}
        ),
    }
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“‹ è¨­å®šæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {config_path}")


def display_saved_info(model) -> None:
    """ä¿å­˜ã•ã‚ŒãŸæƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹"""
    # ä¿å­˜ã•ã‚ŒãŸå­¦ç¿’æƒ…å ±ã‚’ç¢ºèª
    if hasattr(model, "get_training_info"):
        training_info = model.get_training_info()
        print("ğŸ“Š ä¿å­˜ã•ã‚ŒãŸå­¦ç¿’æƒ…å ±:")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {training_info.get('data_size', 'N/A')}")
        print(f"  ç‰¹å¾´é‡æ•°: {training_info.get('feature_count', 'N/A')}")
        print(f"  å­¦ç¿’æ—¥æ™‚: {training_info.get('training_timestamp', 'N/A')}")

    if hasattr(model, "get_comments"):
        comments = model.get_comments()
        if comments:
            print(f"ğŸ“ ä¿å­˜ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆ ({len(comments)}å€‹):")
            # è‡ªå‹•ã‚³ãƒ¡ãƒ³ãƒˆã¨æ‰‹å‹•ã‚³ãƒ¡ãƒ³ãƒˆã‚’åŒºåˆ¥ã—ã¦è¡¨ç¤º
            auto_comments = []
            manual_comments = []

            for comment in comments:
                if any(
                    keyword in comment
                    for keyword in [
                        "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:",
                        "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—:",
                        "ç‰¹å¾´é‡æ•°:",
                        "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:",
                        "åˆ†å‰²æ–¹æ³•:",
                        "æ™‚ç³»åˆ—åˆ—:",
                        "ãƒ¢ãƒ‡ãƒ«:",
                        "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:",
                    ]
                ):
                    auto_comments.append(comment)
                else:
                    manual_comments.append(comment)

            if auto_comments:
                print("  ğŸ”§ è‡ªå‹•ç”Ÿæˆã‚³ãƒ¡ãƒ³ãƒˆ:")
                for i, comment in enumerate(auto_comments, 1):
                    print(f"    {i}. {comment}")

            if manual_comments:
                print("  âœï¸  æ‰‹å‹•å…¥åŠ›ã‚³ãƒ¡ãƒ³ãƒˆ:")
                for i, comment in enumerate(manual_comments, 1):
                    print(f"    {i}. {comment}")
        else:
            print("ğŸ“ ä¿å­˜ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆ: ãªã—")


def save_model(args: Args, pipeline):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
    if not args.no_save:
        try:
            save_path = Path(args.model_save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’ä¿å­˜
            pipeline.save_model(save_path)
            print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")

            # MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜
            save_model_to_mlflow(args, save_path, pipeline)

            # ä¿å­˜ã•ã‚ŒãŸå­¦ç¿’æƒ…å ±ã‚’ç¢ºèª
            model = pipeline.get_model()
            display_saved_info(model)

            # è¨­å®šæƒ…å ±ã‚‚ä¿å­˜(MLflowã‚’ä½¿ç”¨ã—ãªã„å ´åˆ)
            save_config_file(args, save_path, model)

        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("\nâš ï¸  ãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ (--no-save ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")

    if args.use_mlflow:
        mlflow.end_run()
        print("\nâœ… MLflowã®å®Ÿè¡Œã‚’çµ‚äº†ã—ã¾ã—ãŸ")


def main():
    # å¼•æ•°ã®è§£æã¨æ¤œè¨¼
    args = parse_and_validate_args()

    # SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    try:
        print("\nğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        data = load_data_from_sqlite_polars(args.db_path, args.table)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {data.shape}")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±è¡¨ç¤º
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±:")
    print(f"  å½¢çŠ¶: {data.shape}")
    print(f"  åˆ—å: {data.columns}")
    print(f"  ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—: {args.target_column}")
    print(f"  ç‰¹å¾´é‡åˆ—æ•°: {len(args.feature_columns)}")

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    pipeline = train_model(args, data)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    save_model(args, pipeline)

    # MLflowã®ä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
    if args.use_mlflow:
        print("\nğŸ“Š MLflowæƒ…å ±:")
        print(f"  å®Ÿé¨“å: {args.experiment_name}")
        print(f"  å®Ÿè¡Œå: {args.run_name}")
    else:
        print(
            "\nâš ï¸  MLflowã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ "
            "(--no-mlflow ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¾ãŸã¯ --use-mlflow ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“)"
        )

    print("\nâœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")


if __name__ == "__main__":
    main()
