import argparse
import json
import sys
from dataclasses import dataclass
from pathlib import Path

import polars as pl

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.models.implementations.lightgbm_model import LightGBMRegressor
from src.features.managers.feature_manager import FeatureManager
from src.features.managers.target_manager import TargetManager
from src.pipelines.implementations.tree_pipeline import TreeModelPipeline
from src.data.utils.data_loader import (
    load_data_from_sqlite_polars,
    validate_db_path,
)
from src.data.utils.interactive_selector import (
    get_table_info_summary,
    select_table_interactively,
    validate_table_exists,
)


@dataclass
class Args:
    model_path: str
    db_path: str = "data/database.sqlite"
    table: str = None
    output_path: str = "predictions.csv"
    config_path: str = None


def feature_engineering(data: pl.DataFrame, feature_columns: list[str]) -> pl.DataFrame:
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ(å­¦ç¿’æ™‚ã¨åŒã˜å‡¦ç†)"""
    processed_data = data.clone()

    # Convert date string to datetime if date column exists
    if "date" in data.columns:
        processed_data = processed_data.with_columns(
            pl.col("date").str.to_datetime("%Y-%m-%d %H:%M:%S").alias("datetime")
        )

        # Day of week: 0=Monday, 1=Tuesday, ..., 6=Sunday
        processed_data = processed_data.with_columns(pl.col("datetime").dt.weekday().alias("dow"))

    # çµ¦æ–™æ—¥ãŒ 25 æ—¥ä»¥é™ãªã®ã§ã€25 æ—¥ä»¥é™ã‚’æœˆæœ«ã¨ã—ã¦ç‰¹å¾´é‡ã«ã™ã‚‹
    if "date_day" in data.columns:
        processed_data = processed_data.with_columns(
            pl.when(pl.col("date_day") >= 25).then(1).otherwise(0).alias("is_month_end")
        )

    # é€±æœ«
    if "dow" in processed_data.columns:
        processed_data = processed_data.with_columns(
            pl.when(pl.col("dow") >= 4).then(1).otherwise(0).alias("is_weekend")
        )

    # ãƒ©ãƒ³ãƒã‚¿ã‚¤ãƒ ãƒ»ãƒ‡ã‚£ãƒŠãƒ¼ã‚¿ã‚¤ãƒ 
    if "time" in data.columns:
        processed_data = processed_data.with_columns(
            pl.when(pl.col("time").is_in([11, 12, 13])).then(1).otherwise(0).alias("is_lunch")
        )
        processed_data = processed_data.with_columns(
            pl.when(pl.col("time") >= 18).then(1).otherwise(0).alias("is_dinner")
        )

    # æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡åˆ—ã®ã¿ã‚’é¸æŠ
    available_columns = [col for col in feature_columns if col in processed_data.columns]
    processed_data = processed_data.select(available_columns)

    print("ç‰¹å¾´é‡ã®ç¢ºèª:")
    print(processed_data.head())
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡åˆ—: {available_columns}")

    return processed_data


def load_saved_model(model_path: Path) -> tuple[TreeModelPipeline, dict]:
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
    print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_path}")

    if not model_path.exists():
        raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
    model = LightGBMRegressor()
    feature_manager = FeatureManager()
    target_manager = TargetManager(pl.Series("dummy", [0.0]))
    pipeline = TreeModelPipeline(model=model, feature_manager=feature_manager, target_manager=target_manager)

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’èª­ã¿è¾¼ã¿
    pipeline.load_model(model_path)

    # å­¦ç¿’æƒ…å ±ã‚’è¡¨ç¤º
    model = pipeline.get_model()
    if hasattr(model, "print_training_info"):
        model.print_training_info()

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚èª­ã¿è¾¼ã¿
    config_path = model_path.with_suffix(".json")
    config = {}
    if config_path.exists():
        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)
        print(f"ğŸ“‹ è¨­å®šæƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {config_path}")
    else:
        print("âš ï¸  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

    return pipeline, config


def interactive_setup(db_path: Path) -> str:
    """å¯¾è©±çš„ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¨­å®š"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š")
    print("=" * 40)

    # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ
    table_name = select_table_interactively(db_path)
    if not table_name:
        print("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        sys.exit(1)

    print(f"âœ… é¸æŠã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")

    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®è¡¨ç¤º
    table_info = get_table_info_summary(db_path, table_name)
    if table_info:
        print("\nğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±:")
        print(f"  è¡Œæ•°: {table_info['row_count']:,}")
        print(f"  åˆ—æ•°: {table_info['column_count']}")
        print("  åˆ—ã®è©³ç´°:")
        for col_name, col_info in table_info["columns"].items():
            if "error" not in col_info:
                print(
                    f"    {col_name}: {col_info['type']} (NULL: {col_info['null_count']}, ãƒ¦ãƒ‹ãƒ¼ã‚¯: {col_info['unique_count']})"
                )

    return table_name


def validate_features(test_data: pl.DataFrame, required_features: list[str]) -> list[str]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¿…è¦ãªç‰¹å¾´é‡ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    available_features = []
    missing_features = []

    for feature in required_features:
        if feature in test_data.columns:
            available_features.append(feature)
        else:
            missing_features.append(feature)

    if missing_features:
        print(f"âŒ ä»¥ä¸‹ã®ç‰¹å¾´é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_features}")
        print(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(test_data.columns)}")
        return []

    return available_features


def _parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã™ã‚‹"""
    parser = argparse.ArgumentParser(description="ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œ")
    parser.add_argument(
        "--model-path",
        type=str,
        required=True,
        help="ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="data/database.sqlite",
        help="SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/database.sqlite)",
    )
    parser.add_argument(
        "--table",
        type=str,
        help="ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«å (æŒ‡å®šã—ãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ)",
    )
    parser.add_argument(
        "--output-path",
        type=str,
        default="predictions.csv",
        help="äºˆæ¸¬çµæœã®å‡ºåŠ›ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: predictions.csv)",
    )
    parser.add_argument(
        "--config-path",
        type=str,
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (æŒ‡å®šã—ãªã„å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜å ´æ‰€ã®.jsonãƒ•ã‚¡ã‚¤ãƒ«)",
    )
    return parser.parse_args(namespace=Args)


def _load_model_and_config(args):
    """ãƒ¢ãƒ‡ãƒ«ã¨è¨­å®šã‚’èª­ã¿è¾¼ã‚€"""
    model_path = Path(args.model_path)
    try:
        pipeline, config = load_saved_model(model_path)
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return pipeline, config
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


def _setup_database_and_table(args, db_path):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®š"""
    # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ(ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ)
    if not args.table:
        table_name = interactive_setup(db_path)
        args.table = table_name
    else:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå ´åˆã®æ¤œè¨¼
        if not validate_table_exists(db_path, args.table):
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« '{args.table}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            sys.exit(1)


def _load_and_validate_data(args, config):
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ¤œè¨¼ã™ã‚‹"""
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    try:
        print("\nğŸ“¥ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        test_data = load_data_from_sqlite_polars(args.db_path, args.table)
        print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {test_data.shape}")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±è¡¨ç¤º
    print("\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±:")
    print(f"  å½¢çŠ¶: {test_data.shape}")
    print(f"  åˆ—å: {test_data.columns}")

    # è¨­å®šã‹ã‚‰ç‰¹å¾´é‡åˆ—ã‚’å–å¾—
    feature_columns = config.get("feature_columns", [])
    if not feature_columns:
        print("âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ç‰¹å¾´é‡åˆ—ã®æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)

    print(f"  å¿…è¦ãªç‰¹å¾´é‡åˆ—: {feature_columns}")

    # ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèª
    available_features = validate_features(test_data, feature_columns)
    if not available_features:
        print("âŒ å¿…è¦ãªç‰¹å¾´é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        sys.exit(1)

    return test_data, available_features


def _execute_prediction(pipeline, test_data, available_features):
    """äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹"""
    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("\nğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    engineered_data = feature_engineering(test_data, available_features)

    # äºˆæ¸¬å®Ÿè¡Œ
    print("\nğŸ”„ äºˆæ¸¬å®Ÿè¡Œä¸­...")
    try:
        model = pipeline.get_model()
        predictions = model.predict(engineered_data)
        print(f"âœ… äºˆæ¸¬å®Œäº†: {len(predictions)}ä»¶")
        return model, predictions
    except Exception as e:
        print(f"âŒ äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


def _save_and_display_results(args, test_data, predictions, model):
    """çµæœã‚’ä¿å­˜ã—ã€è¡¨ç¤ºã™ã‚‹"""
    # äºˆæ¸¬çµæœã®è¡¨ç¤º
    print("\nğŸ“ˆ äºˆæ¸¬çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  äºˆæ¸¬ä»¶æ•°: {len(predictions)}")
    print(f"  å¹³å‡äºˆæ¸¬å€¤: {predictions.mean():.4f}")
    print(f"  æœ€å°äºˆæ¸¬å€¤: {predictions.min():.4f}")
    print(f"  æœ€å¤§äºˆæ¸¬å€¤: {predictions.max():.4f}")
    print(f"  æ¨™æº–åå·®: {predictions.std():.4f}")

    # äºˆæ¸¬çµæœã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨çµåˆ
    result_data = test_data.with_columns(predictions.alias("prediction"))

    # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
    print("\nğŸ“‹ äºˆæ¸¬çµæœ(æœ€åˆã®10ä»¶):")
    print(result_data.select(["prediction", *list(test_data.columns[:5])]).head(10))

    # çµæœã‚’CSVã«ä¿å­˜
    try:
        output_path = Path(args.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        result_data.write_csv(output_path)
        print(f"\nğŸ’¾ äºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    except Exception as e:
        print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¡¨ç¤º(ãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã—ã¦ã„ã‚‹å ´åˆ)
    try:
        importance = model.get_feature_importance()
        print("\nğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦ (ãƒˆãƒƒãƒ—10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
        for feature, imp in sorted_importance[:10]:
            print(f"  {feature}: {imp:.4f}")
    except Exception as e:
        print(f"âš ï¸  ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    args = _parse_arguments()

    print("ğŸš€ ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸäºˆæ¸¬å®Ÿè¡Œ")
    print("=" * 60)
    print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {args.model_path}")
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: {args.db_path}")

    # ãƒ¢ãƒ‡ãƒ«ã¨è¨­å®šã‚’èª­ã¿è¾¼ã¿
    pipeline, config = _load_model_and_config(args)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å­˜åœ¨ç¢ºèª
    db_path = Path(args.db_path)
    if not validate_db_path(db_path):
        sys.exit(1)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨­å®š
    _setup_database_and_table(args, db_path)

    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ¤œè¨¼
    test_data, available_features = _load_and_validate_data(args, config)

    # äºˆæ¸¬ã‚’å®Ÿè¡Œ
    model, predictions = _execute_prediction(pipeline, test_data, available_features)

    # çµæœã‚’ä¿å­˜ã—ã€è¡¨ç¤º
    _save_and_display_results(args, test_data, predictions, model)

    print("\nâœ… äºˆæ¸¬å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")


if __name__ == "__main__":
    main()
